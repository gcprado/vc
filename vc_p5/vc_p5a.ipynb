{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f3c2d",
   "metadata": {},
   "source": [
    "Organizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83641682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning UTKFace image files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10137/10137 [00:00<00:00, 1460987.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid images found: 10137\n",
      "Classes: [np.str_('anciano'), np.str_('joven'), np.str_('medio')]\n",
      "\n",
      "Saving train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7095/7095 [01:07<00:00, 105.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [00:14<00:00, 107.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [00:15<00:00, 99.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset ready!\n",
      "Output directory: dataset_by_age2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "INPUT_DIR = \"dataset\"\n",
    "OUTPUT_DIR = \"dataset_by_age2\"\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LABEL EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "def extract_age_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract age from UTKFace-style filenames:\n",
    "    - '1_0_0_20161219140623097.jpg'\n",
    "    - '1_0_0_20161219140623097.jpg.chip.jpg'\n",
    "    - '12_1_3_201701022015.png'\n",
    "    \n",
    "    Age is ALWAYS the first integer before the first underscore.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        age_str = filename.split(\"_\")[0]\n",
    "        return int(age_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def age_to_category(age):\n",
    "    if age < 40:\n",
    "        return \"joven\"\n",
    "    elif age < 65:\n",
    "        return \"medio\"\n",
    "    else:\n",
    "        return \"anciano\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMAGE PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def process_image(path):\n",
    "    \"\"\"Load, resize, and return a processed PIL image.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    return img\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DIRECTORY CREATION\n",
    "# ============================================================\n",
    "\n",
    "def recreate_structure(base_dir, classes):\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            Path(base_dir, split, cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SCAN DATASET\n",
    "# ============================================================\n",
    "\n",
    "print(\"Scanning UTKFace image files...\")\n",
    "\n",
    "input_dir = Path(INPUT_DIR)\n",
    "files = [f for f in input_dir.iterdir() if f.is_file()]\n",
    "\n",
    "records = []\n",
    "for f in tqdm(files):\n",
    "    age = extract_age_from_filename(f.name)\n",
    "    if age is None:\n",
    "        continue\n",
    "    category = age_to_category(age)\n",
    "    records.append((f, category))\n",
    "\n",
    "print(f\"Valid images found: {len(records)}\")\n",
    "\n",
    "paths = np.array([r[0] for r in records])\n",
    "labels = np.array([r[1] for r in records])\n",
    "\n",
    "classes = sorted(list(set(labels)))\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATIFIED SPLIT\n",
    "# ============================================================\n",
    "\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    paths, labels,\n",
    "    test_size=0.30,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "splits = {\n",
    "    \"train\": list(zip(train_paths, train_labels)),\n",
    "    \"validation\": list(zip(val_paths, val_labels)),\n",
    "    \"test\": list(zip(test_paths, test_labels)),\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ENABLE DIRECTORY STRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "recreate_structure(OUTPUT_DIR, classes)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE IMAGES STREAMING (UUID + PNG)\n",
    "# ============================================================\n",
    "\n",
    "def save_split(name, split_data, output_base):\n",
    "    print(f\"\\nSaving {name}...\")\n",
    "    for path, label in tqdm(split_data):\n",
    "        img = process_image(path)\n",
    "\n",
    "        # generate random UUID filename\n",
    "        new_name = f\"{uuid.uuid4().hex}.png\"\n",
    "\n",
    "        dest = Path(output_base) / name / label / new_name\n",
    "\n",
    "        img.save(dest, format=\"PNG\")\n",
    "\n",
    "\n",
    "save_split(\"train\", splits[\"train\"], OUTPUT_DIR)\n",
    "save_split(\"validation\", splits[\"validation\"], OUTPUT_DIR)\n",
    "save_split(\"test\", splits[\"test\"], OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nDataset ready!\")\n",
    "print(\"Output directory:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66ea3d",
   "metadata": {},
   "source": [
    "Crear una version normalizada (igual cantidad de muestras por clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ea5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes por clase tras normalizar: 1148\n",
      "Dataset balanceado y normalizado creado en: dataset_by_age_normalized2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================\n",
    "# Configuración\n",
    "# ==============================\n",
    "INPUT_DIR = \"dataset_by_age2\"\n",
    "OUTPUT_DIR = \"dataset_by_age_normalized2\"\n",
    "\n",
    "SPLITS = ['train', 'validation', 'test']\n",
    "CLASSES = ['joven', 'medio', 'anciano']\n",
    "SPLIT_RATIOS = {'train': 0.7, 'validation': 0.15, 'test': 0.15}\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ==============================\n",
    "# Funciones\n",
    "# ==============================\n",
    "def make_dirs(base_dir):\n",
    "    for split in SPLITS:\n",
    "        for cls in CLASSES:\n",
    "            Path(base_dir, split, cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_all_files(input_dir):\n",
    "    \"\"\"Devuelve un diccionario: class -> list of files\"\"\"\n",
    "    class_files = {}\n",
    "    for cls in CLASSES:\n",
    "        files = list(Path(input_dir, 'train', cls).glob(\"*.png\")) + \\\n",
    "                list(Path(input_dir, 'validation', cls).glob(\"*.png\")) + \\\n",
    "                list(Path(input_dir, 'test', cls).glob(\"*.png\"))\n",
    "        class_files[cls] = files\n",
    "    return class_files\n",
    "\n",
    "def split_files(files, ratios):\n",
    "    \"\"\"Divide lista de archivos en train/val/test según ratios\"\"\"\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_train = int(ratios['train']*n)\n",
    "    n_val = int(ratios['validation']*n)\n",
    "    train_files = files[:n_train]\n",
    "    val_files = files[n_train:n_train+n_val]\n",
    "    test_files = files[n_train+n_val:]\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# ==============================\n",
    "# Pipeline principal\n",
    "# ==============================\n",
    "\n",
    "# Crear estructura de carpetas\n",
    "make_dirs(OUTPUT_DIR)\n",
    "\n",
    "# Obtener todos los archivos por clase\n",
    "class_files = get_all_files(INPUT_DIR)\n",
    "\n",
    "# Determinar tamaño de la clase minoritaria\n",
    "min_count = min(len(files) for files in class_files.values())\n",
    "print(\"Cantidad de imágenes por clase tras normalizar:\", min_count)\n",
    "\n",
    "# Para cada clase, tomar solo min_count imágenes y dividir en splits\n",
    "for cls, files in class_files.items():\n",
    "    selected_files = random.sample(files, min_count)  # submuestreo\n",
    "    train_files, val_files, test_files = split_files(selected_files, SPLIT_RATIOS)\n",
    "\n",
    "    # Copiar archivos a la nueva estructura\n",
    "    for f in train_files:\n",
    "        shutil.copy(f, Path(OUTPUT_DIR, 'train', cls, f.name))\n",
    "    for f in val_files:\n",
    "        shutil.copy(f, Path(OUTPUT_DIR, 'validation', cls, f.name))\n",
    "    for f in test_files:\n",
    "        shutil.copy(f, Path(OUTPUT_DIR, 'test', cls, f.name))\n",
    "\n",
    "print(\"Dataset balanceado y normalizado creado en:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
